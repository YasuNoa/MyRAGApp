//
//  VoiceNoteViewModel.swift
//  jibunAI-ios
//
//  Created by Automation on 2026/01/28.
//

import Foundation
import Combine
import AVFoundation
import Speech
import SwiftUI

@MainActor
class VoiceNoteViewModel: NSObject, ObservableObject, SFSpeechRecognizerDelegate {
    
    // MARK: - Published Properties
    
    @Published var isRecording = false
    @Published var recordingTime: TimeInterval = 0
    
    // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜çµæœ
    @Published var transcript = ""
    
    // è§£æçµæœ
    @Published var summary = ""
    @Published var tags: [String] = []
    
    // çŠ¶æ…‹
    @Published var isProcessing = false
    @Published var errorMessage: String?
    @Published var showLimitAlert = false
    @Published var limitAlertMessage: String? // ã‚¢ãƒ©ãƒ¼ãƒˆç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    // MARK: - Private Properties
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ™‚ä¿å­˜å ´æ‰€
    private var audioFileURL: URL?
    private var audioFile: AVAudioFile?
    
    private var timer: Timer?
    
    // API
    private let apiService = APIService.shared
    
    // MARK: - Singleton
    
    static let shared = VoiceNoteViewModel()
    
    private override init() {
        super.init()
        speechRecognizer?.delegate = self
    }
    
    
    /// ãƒ­ã‚°ã‚¢ã‚¦ãƒˆæ™‚ãªã©ã«å‘¼ã³å‡ºã™ã€‚éŒ²éŸ³åœæ­¢ã—ã€æœªä¿å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã‚µãƒ¼ãƒãƒ¼é€ä¿¡ã‚’è©¦ã¿ã‚‹
    /// æ³¨æ„: ã“ã®å‡¦ç†ã¯éåŒæœŸã§ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…æ©Ÿã—ã¾ã™
    func cleanup(userId: String) async {
        if isRecording {
            AppLogger.general.debug("ğŸ›‘ Force stopping recording due to cleanup")
            stopRecordingInternal() // éŒ²éŸ³åœæ­¢ï¼ˆAPIã‚³ãƒ¼ãƒ«ãªã—ç‰ˆï¼‰
        }
        
        // é€”ä¸­ã¾ã§ã® transcript ãŒã‚ã‚Œã°ä¿å­˜ã‚’è©¦ã¿ã‚‹
        if !transcript.isEmpty {
            let currentTranscript = transcript
            AppLogger.general.info("ğŸ’¾ Auto-saving partial transcript \(currentTranscript.count) chars...")
            
            do {
                // è¦ç´„ãªã—ã§ä¿å­˜
                let _ = try await apiService.saveVoice(
                    userId: userId,
                    transcript: currentTranscript,
                    summary: "ã€è‡ªå‹•ä¿å­˜ã€‘é€”ä¸­ã¾ã§ã®è¨˜éŒ²",
                    title: "Voice Memo (Auto-saved) \(Date().formatted())",
                    tags: ["auto-saved"]
                )
                AppLogger.general.info("âœ… Auto-save successful")
            } catch {
                 AppLogger.general.error("âš ï¸ Auto-save failed: \(error)")
            }
        }
        
        reset()
    }
    
    /// å†…éƒ¨ç”¨: APIã‚³ãƒ¼ãƒ«ã‚’è¡Œã‚ãšã«éŒ²éŸ³ã ã‘æ­¢ã‚ã‚‹
    private func stopRecordingInternal() {
        audioEngine.stop()
        if audioEngine.inputNode.numberOfInputs > 0 {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        
        isRecording = false
        timer?.invalidate()
        timer = nil
        audioFile = nil
    }
    
    // MARK: - Recording Methods
    
    func startRecording() {
        // å‰å›ã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Œã°ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        if recognitionTask != nil {
            recognitionTask?.cancel()
            recognitionTask = nil
        }
        
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            errorMessage = "éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true
        
        // éŒ²éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜æº–å‚™
        let docDir = FileManager.default.temporaryDirectory
        let fileName = "recording_\(Date().timeIntervalSince1970).caf" // .m4a -> .caf (Linear PCM)
        audioFileURL = docDir.appendingPathComponent(fileName)
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // AVAudioFileã®ä½œæˆ (æ›¸ãè¾¼ã¿ç”¨)
        do {
            guard let url = audioFileURL else {
                 AppLogger.general.error("AudioFile init error: URL is nil")
                 return
            }
            // Note: AVAudioFile using settings from inputNode
            // Cloud Runã®æ–‡å­—èµ·ã“ã—API (Whisperç­‰) ã¯m4a/mp3ãªã©ã‚’æœŸå¾…ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€
            // CoreAudioã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã‚‹ãŒã€ã“ã“ã§ã¯cafãªã©æ‰±ã„ã‚„ã™ã„å½¢å¼ã§ä¸€æ—¦ä¿å­˜ã—ã€
            // ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«å¤‰æ›ã™ã‚‹ã‹ã€ãã®ã¾ã¾é€ã‚‹ã€‚
            // ç°¡æ˜“çš„ã« .caf (Core Audio Format) ã§ä¿å­˜ã™ã‚‹ã€‚
            audioFile = try AVAudioFile(forWriting: url, settings: recordingFormat.settings)
        } catch {
            AppLogger.general.error("AudioFile init error: \(error)")
        }
        
        // èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            var isFinal = false
            
            if let result = result {
                self.transcript = result.bestTranscription.formattedString
                isFinal = result.isFinal
            }
            
            if error != nil || isFinal {
                self.audioEngine.stop()
                inputNode.removeTap(onBus: 0)
                
                self.recognitionRequest = nil
                self.recognitionTask = nil
                
                // éŒ²éŸ³çµ‚äº†æ™‚ã®å‡¦ç†ã¯ stopRecording ã§è¡Œã†ãŸã‚ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„
            }
        }
        
        // ãƒã‚¤ã‚¯å…¥åŠ›ã®ã‚¿ãƒƒãƒ—
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] (buffer, when) in
            guard let self = self else { return }
            
            // éŸ³å£°èªè­˜ã¸
            self.recognitionRequest?.append(buffer)
            
            // ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¸
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                AppLogger.general.error("Audio write error: \(error)")
            }
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            isRecording = true
            errorMessage = nil
            transcript = ""
            summary = ""
            tags = []
            
            // ã‚¿ã‚¤ãƒãƒ¼ã‚¹ã‚¿ãƒ¼ãƒˆ
            recordingTime = 0
            timer = Timer.scheduledTimer(withTimeInterval: 1, repeats: true) { _ in
                Task { @MainActor in
                    self.recordingTime += 1
                }
            }
        } catch {
            errorMessage = "éŒ²éŸ³ã‚¨ãƒ³ã‚¸ãƒ³ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
        }
    }
    
    func stopRecording(userId: String) {
        // APIå‡¦ç†ã‚’å‘¼ã³å‡ºã™é€šå¸¸ã®åœæ­¢å‡¦ç†
        stopRecordingInternal()
        
        Task {
            // å°‘ã—å¾…ã£ã¦ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼†è§£æé–‹å§‹
             try? await Task.sleep(nanoseconds: 500_000_000) // 0.5s
             await processAudio(userId: userId)
        }
    }

    
    // APIã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æ
    private func processAudio(userId: String) async {
        guard let fileURL = audioFileURL else { return }
        
        isProcessing = true
        
        do {
            // ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼†å‡¦ç†
             let response = try await apiService.processVoice(
                fileURL: fileURL,
                fileName: "voice_memo.caf", // .m4a -> .caf (Backend supports .caf)
                userId: userId,
                tags: self.tags
            )
            
            self.transcript = response.transcript
            self.summary = response.summary
            
        } catch {
             // 403 Forbidden (åˆ¶é™åˆ°é”) ã®å ´åˆ
             if let apiError = error as? APIError, case .forbidden(let detail) = apiError {
                  AppLogger.general.warning("Voice Limit Reached: \(detail)")
                  // æœˆé–“åˆ¶é™ã‹åˆ¤å®š (ä»Šå›ã¯ç´°ã‹ãåˆ†ã‘ãšã¨ã‚‚APIã‹ã‚‰ã®detailã‚’ä½¿ã†ã‹ã€å›ºå®šæ–‡è¨€ã§ã‚‚è‰¯ã„ãŒã€æœˆé–“åˆ¶é™ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã®ã§æœˆé–“ã¨ã™ã‚‹)
                  limitAlertMessage = "Freeãƒ—ãƒ©ãƒ³ã®æœˆé–“éŸ³å£°å‡¦ç†ä¸Šé™ï¼ˆ5æ™‚é–“ï¼‰ã«é”ã—ã¾ã—ãŸã€‚\nç„¡åˆ¶é™ãƒ—ãƒ©ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
                  showLimitAlert = true
             } else {
                  errorMessage = "è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
             }
        }
        
        isProcessing = false
    }
    
    // è§£æçµæœã‚’ä¿å­˜ (Webã® "Import")
    func saveVoice(userId: String) async -> Bool {
        do {
            let _ = try await apiService.saveVoice(
                userId: userId,
                transcript: transcript,
                summary: summary,
                title: "Voice Memo \(Date().formatted())", // ã‚¿ã‚¤ãƒˆãƒ«ã¯æ—¥æ™‚ã§è‡ªå‹•ç”Ÿæˆ
                tags: tags
            )
            
            // æˆåŠŸã—ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
            reset()
            return true
        } catch {
            errorMessage = "ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: \(error.localizedDescription)"
            return false
        }
    }
    
    func reset() {
        transcript = ""
        summary = ""
        tags = []
        errorMessage = nil
        recordingTime = 0
        isProcessing = false
        // ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if let url = audioFileURL {
            try? FileManager.default.removeItem(at: url)
            audioFileURL = nil
        }
    }
}
